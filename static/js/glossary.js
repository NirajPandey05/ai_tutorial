/**
 * Comprehensive AI/ML Glossary System
 * Features: Search, filtering, cross-references, categories, inline tooltips
 */

const GlossaryData = {
    terms: [
        // === A ===
        {
            term: "Agent",
            definition: "An AI system that can perceive its environment, make decisions, and take actions to achieve goals autonomously. Agents typically use LLMs for reasoning and can interact with tools and APIs.",
            category: "agents",
            relatedTerms: ["Multi-Agent System", "Tool Use", "ReAct", "Function Calling"],
            seeAlso: ["/learn/agents"]
        },
        {
            term: "Attention Mechanism",
            definition: "A technique in neural networks that allows models to focus on relevant parts of the input when generating output. Self-attention is the key innovation in transformer architecture, enabling parallel processing and capturing long-range dependencies.",
            category: "architecture",
            relatedTerms: ["Transformer", "Self-Attention", "Multi-Head Attention", "Context Window"],
            seeAlso: ["/learn/advanced-llm/attention-mechanisms"]
        },
        {
            term: "API Key",
            definition: "A unique identifier used to authenticate requests to an LLM provider's API. Keys should be kept secret and never committed to version control.",
            category: "infrastructure",
            relatedTerms: ["Rate Limiting", "Token"],
            seeAlso: ["/settings"]
        },
        
        // === B ===
        {
            term: "BERT",
            definition: "Bidirectional Encoder Representations from Transformers. A pre-trained language model from Google that processes text bidirectionally, making it excellent for understanding tasks like classification and question answering.",
            category: "models",
            relatedTerms: ["Transformer", "Embedding", "GPT"],
            seeAlso: ["/learn/llm-fundamentals/intro-to-llms/history"]
        },
        {
            term: "BPE (Byte Pair Encoding)",
            definition: "A tokenization algorithm that iteratively merges the most frequent pairs of characters or tokens. Used by GPT models to convert text into tokens.",
            category: "tokens",
            relatedTerms: ["Token", "Tokenizer", "SentencePiece"],
            seeAlso: ["/learn/llm-fundamentals/tokens"]
        },
        
        // === C ===
        {
            term: "Chain-of-Thought (CoT)",
            definition: "A prompting technique that encourages the model to show its reasoning step-by-step before providing a final answer. Improves accuracy on complex reasoning tasks by making the thinking process explicit.",
            category: "prompting",
            relatedTerms: ["Zero-shot CoT", "Few-shot Learning", "Prompt Engineering"],
            seeAlso: ["/learn/llm-fundamentals/prompt-engineering"]
        },
        {
            term: "Chunking",
            definition: "The process of splitting documents into smaller pieces for processing in RAG systems. Chunk size and overlap significantly impact retrieval quality. Common strategies include fixed-size, sentence-based, and semantic chunking.",
            category: "rag",
            relatedTerms: ["RAG", "Embedding", "Vector Database", "Context Window"],
            seeAlso: ["/learn/rag/chunking-strategies"]
        },
        {
            term: "Claude",
            definition: "A family of large language models developed by Anthropic, known for being helpful, harmless, and honest. Latest versions include Claude Opus 4.5 (#1 on leaderboards), Claude Sonnet 4.5, and Claude 4 series.",
            category: "models",
            relatedTerms: ["LLM", "Anthropic", "Constitutional AI"],
            seeAlso: ["/learn/llm-fundamentals/intro-to-llms"]
        },
        {
            term: "Completion",
            definition: "The text generated by an LLM in response to a prompt. The completion API endpoint is used for text generation tasks.",
            category: "basics",
            relatedTerms: ["Prompt", "Token", "Inference"],
            seeAlso: ["/learn/llm-fundamentals/intro-to-llms/first-api-call"]
        },
        {
            term: "Constitutional AI",
            definition: "A training approach developed by Anthropic where AI systems are trained to follow a set of principles (a 'constitution') through self-critique and revision.",
            category: "training",
            relatedTerms: ["RLHF", "Fine-tuning", "Claude"],
            seeAlso: ["/learn/fine-tuning"]
        },
        {
            term: "Context Window",
            definition: "The maximum number of tokens an LLM can process in a single request, including both input and output. Modern models range from 128K to 4M+ tokens. Gemini 3 supports 1M tokens, GPT-5 supports 400K.",
            category: "basics",
            relatedTerms: ["Token", "Prompt", "RAG"],
            seeAlso: ["/learn/llm-fundamentals/tokens/context-limits"]
        },
        {
            term: "Cosine Similarity",
            definition: "A measure of similarity between two vectors, calculated as the cosine of the angle between them. Values range from -1 to 1, with 1 being identical. Commonly used to compare embeddings in semantic search.",
            category: "embeddings",
            relatedTerms: ["Embedding", "Semantic Search", "Vector Database"],
            seeAlso: ["/learn/embeddings-vectors/similarity-search"]
        },
        
        // === D ===
        {
            term: "Dense Retrieval",
            definition: "A retrieval method using dense vector embeddings to find semantically similar documents, as opposed to sparse methods like BM25 that rely on keyword matching.",
            category: "rag",
            relatedTerms: ["Embedding", "Semantic Search", "BM25", "Hybrid Search"],
            seeAlso: ["/learn/rag/basic-retrieval"]
        },
        
        // === E ===
        {
            term: "Embedding",
            definition: "A dense vector representation of text (or other data) that captures semantic meaning in a high-dimensional space. Similar concepts have similar embeddings, enabling semantic search and clustering.",
            category: "embeddings",
            relatedTerms: ["Vector Database", "Semantic Search", "Cosine Similarity"],
            seeAlso: ["/learn/embeddings-vectors/what-are-embeddings"]
        },
        {
            term: "Embedding Model",
            definition: "A model specifically designed to convert text into embeddings. Examples include OpenAI's text-embedding-3-small, Cohere's embed-v3, and open-source models like E5 and BGE.",
            category: "embeddings",
            relatedTerms: ["Embedding", "Vector Database", "Semantic Search"],
            seeAlso: ["/learn/embeddings-vectors/embedding-models"]
        },
        
        // === F ===
        {
            term: "Few-shot Learning",
            definition: "Providing a few examples in the prompt to guide the model's behavior without fine-tuning. Typically 2-5 examples are included to demonstrate the desired input-output pattern.",
            category: "prompting",
            relatedTerms: ["Zero-shot Learning", "One-shot Learning", "In-Context Learning"],
            seeAlso: ["/learn/llm-fundamentals/prompt-engineering/zero-few-shot"]
        },
        {
            term: "Fine-tuning",
            definition: "Training a pre-trained model on a specific dataset to adapt it for a particular task or domain. Creates a specialized model that performs better on targeted use cases.",
            category: "training",
            relatedTerms: ["LoRA", "QLoRA", "RLHF", "Pre-training"],
            seeAlso: ["/learn/fine-tuning"]
        },
        {
            term: "Function Calling",
            definition: "The ability of an LLM to output structured data (usually JSON) that represents a call to an external function or tool. Enables LLMs to interact with APIs, databases, and other systems.",
            category: "agents",
            relatedTerms: ["Tool Use", "Agent", "JSON Mode"],
            seeAlso: ["/learn/agents/tool-use"]
        },
        
        // === G ===
        {
            term: "Gemini",
            definition: "Google's family of multimodal AI models, capable of processing text, images, audio, and video. Gemini 3 Pro ranks #2 on leaderboards with 1M token context. Includes Pro, Flash, and Ultra variants.",
            category: "models",
            relatedTerms: ["LLM", "Multimodal", "Google"],
            seeAlso: ["/learn/llm-fundamentals/intro-to-llms"]
        },
        {
            term: "GPT",
            definition: "Generative Pre-trained Transformer. A family of language models from OpenAI. GPT-5.2 is the latest flagship model (92.4% GPQA), with GPT-5.1 and GPT-4.1 also available.",
            category: "models",
            relatedTerms: ["OpenAI", "Transformer", "LLM"],
            seeAlso: ["/learn/llm-fundamentals/intro-to-llms"]
        },
        {
            term: "Grounding",
            definition: "Connecting LLM outputs to factual, verifiable information sources. RAG is a common grounding technique that retrieves relevant documents to inform responses.",
            category: "rag",
            relatedTerms: ["RAG", "Hallucination", "Citation"],
            seeAlso: ["/learn/rag/citation-tracking"]
        },
        
        // === H ===
        {
            term: "Hallucination",
            definition: "When an LLM generates plausible-sounding but factually incorrect or nonsensical information. A major challenge in production AI systems, mitigated through RAG, grounding, and verification.",
            category: "challenges",
            relatedTerms: ["Grounding", "RAG", "Factuality"],
            seeAlso: ["/learn/rag/what-is-rag"]
        },
        {
            term: "Hybrid Search",
            definition: "Combining dense vector search with sparse keyword search (like BM25) to get the benefits of both semantic understanding and exact matching.",
            category: "rag",
            relatedTerms: ["Dense Retrieval", "BM25", "Reranking"],
            seeAlso: ["/learn/rag/hybrid-search"]
        },
        {
            term: "HyDE",
            definition: "Hypothetical Document Embeddings. A RAG technique where the LLM generates a hypothetical answer first, which is then embedded and used for retrieval instead of the original query.",
            category: "rag",
            relatedTerms: ["RAG", "Query Expansion", "Embedding"],
            seeAlso: ["/learn/rag/multi-query"]
        },
        
        // === I ===
        {
            term: "In-Context Learning",
            definition: "The ability of LLMs to learn new tasks from examples provided in the prompt without updating model weights. Enables few-shot and zero-shot capabilities.",
            category: "prompting",
            relatedTerms: ["Few-shot Learning", "Zero-shot Learning", "Prompt Engineering"],
            seeAlso: ["/learn/llm-fundamentals/prompt-engineering"]
        },
        {
            term: "Inference",
            definition: "The process of using a trained model to generate predictions or outputs. In LLMs, this means generating text based on a prompt.",
            category: "basics",
            relatedTerms: ["Token", "Completion", "Latency"],
            seeAlso: ["/learn/llm-fundamentals/intro-to-llms/how-they-work"]
        },
        
        // === J ===
        {
            term: "JSON Mode",
            definition: "A setting that constrains the LLM to output valid JSON, useful for structured data extraction and function calling scenarios.",
            category: "basics",
            relatedTerms: ["Function Calling", "Structured Output"],
            seeAlso: ["/learn/llm-fundamentals/parameters"]
        },
        
        // === K ===
        {
            term: "Knowledge Cutoff",
            definition: "The date up to which an LLM was trained on data. The model has no knowledge of events after this date unless provided through RAG or context.",
            category: "basics",
            relatedTerms: ["RAG", "Context Window", "Grounding"],
            seeAlso: ["/learn/rag/what-is-rag"]
        },
        
        // === L ===
        {
            term: "LangChain",
            definition: "A popular framework for building LLM applications that provides abstractions for chains, agents, memory, and retrieval. Supports multiple LLM providers.",
            category: "frameworks",
            relatedTerms: ["LlamaIndex", "Agent", "RAG"],
            seeAlso: ["/learn/agents"]
        },
        {
            term: "LLM (Large Language Model)",
            definition: "An AI model trained on massive text data to understand and generate human language. Top models include Claude Opus 4.5, Gemini 3 Pro, GPT-5.2, and open-source options like DeepSeek-V3.2 and Qwen3.",
            category: "basics",
            relatedTerms: ["Transformer", "Token", "Fine-tuning"],
            seeAlso: ["/learn/llm-fundamentals/intro-to-llms/what-are-llms"]
        },
        {
            term: "Llama",
            definition: "Meta's family of open-source large language models. Llama 4 is the latest version, available for commercial use and self-hosting. Competes with DeepSeek and Qwen in open-source space.",
            category: "models",
            relatedTerms: ["Open Source", "Self-hosting", "Quantization"],
            seeAlso: ["/learn/self-hosting/open-source-models"]
        },
        {
            term: "LlamaIndex",
            definition: "A data framework for building LLM applications, focused on ingesting, structuring, and accessing private or domain-specific data for RAG systems.",
            category: "frameworks",
            relatedTerms: ["LangChain", "RAG", "Vector Database"],
            seeAlso: ["/learn/rag"]
        },
        {
            term: "LoRA",
            definition: "Low-Rank Adaptation. A parameter-efficient fine-tuning technique that adds small trainable matrices to frozen model weights, dramatically reducing memory and compute requirements.",
            category: "training",
            relatedTerms: ["Fine-tuning", "QLoRA", "PEFT"],
            seeAlso: ["/learn/fine-tuning/types-of-finetuning"]
        },
        
        // === M ===
        {
            term: "MCP (Model Context Protocol)",
            definition: "An open protocol for connecting AI assistants to external data sources and tools. Enables standardized integration between LLMs and various services.",
            category: "infrastructure",
            relatedTerms: ["Tool Use", "Function Calling", "Agent"],
            seeAlso: ["/learn/mcp"]
        },
        {
            term: "Mixture of Experts (MoE)",
            definition: "An architecture where multiple specialized 'expert' networks are combined, with a gating mechanism that routes inputs to the most relevant experts. Enables larger models with efficient inference.",
            category: "architecture",
            relatedTerms: ["Transformer", "GPT-4", "Sparse Model"],
            seeAlso: ["/learn/advanced-llm/moe-models"]
        },
        {
            term: "Multi-Agent System",
            definition: "A system where multiple AI agents collaborate to solve complex tasks. Agents may have different roles, share information, and coordinate their actions.",
            category: "agents",
            relatedTerms: ["Agent", "Orchestration", "Swarm Intelligence"],
            seeAlso: ["/learn/multi-agents"]
        },
        {
            term: "Multimodal",
            definition: "AI systems that can process and generate multiple types of data (text, images, audio, video). Gemini 3, GPT-5.2, and Claude Opus 4.5 all support multimodal inputs.",
            category: "basics",
            relatedTerms: ["Vision Language Model", "Gemini", "GPT-5"],
            seeAlso: ["/learn/advanced-llm/multimodal-llms"]
        },
        
        // === O ===
        {
            term: "Ollama",
            definition: "An open-source tool for running LLMs locally. Provides easy installation and management of models like Llama, Mistral, and others on personal hardware.",
            category: "infrastructure",
            relatedTerms: ["Self-hosting", "Llama", "Quantization"],
            seeAlso: ["/learn/self-hosting/ollama-setup"]
        },
        {
            term: "One-shot Learning",
            definition: "Providing a single example in the prompt to guide the model's behavior. A middle ground between zero-shot and few-shot approaches.",
            category: "prompting",
            relatedTerms: ["Zero-shot Learning", "Few-shot Learning", "In-Context Learning"],
            seeAlso: ["/learn/llm-fundamentals/prompt-engineering/zero-few-shot"]
        },
        
        // === P ===
        {
            term: "Prompt",
            definition: "The input text given to an LLM to generate a response. Can include instructions, context, examples, and the specific question or task.",
            category: "basics",
            relatedTerms: ["Completion", "System Prompt", "Prompt Engineering"],
            seeAlso: ["/learn/llm-fundamentals/prompt-engineering"]
        },
        {
            term: "Prompt Engineering",
            definition: "The practice of crafting effective prompts to get desired outputs from LLMs. Includes techniques like few-shot learning, chain-of-thought, and structured prompts.",
            category: "prompting",
            relatedTerms: ["Chain-of-Thought", "Few-shot Learning", "System Prompt"],
            seeAlso: ["/learn/llm-fundamentals/prompt-engineering"]
        },
        {
            term: "Prompt Injection",
            definition: "An attack where malicious instructions are inserted into prompts to manipulate LLM behavior, potentially bypassing safety measures or extracting sensitive information.",
            category: "challenges",
            relatedTerms: ["Security", "System Prompt", "Jailbreak"],
            seeAlso: ["/learn/agents/security-safety"]
        },
        
        // === Q ===
        {
            term: "QLoRA",
            definition: "Quantized LoRA. Combines LoRA with 4-bit quantization to enable fine-tuning of large models on consumer GPUs with minimal quality loss.",
            category: "training",
            relatedTerms: ["LoRA", "Quantization", "Fine-tuning"],
            seeAlso: ["/learn/fine-tuning/local-finetuning"]
        },
        {
            term: "Quantization",
            definition: "Reducing the precision of model weights (e.g., from 16-bit to 4-bit) to decrease memory usage and increase inference speed, with some quality trade-off.",
            category: "infrastructure",
            relatedTerms: ["Self-hosting", "GGUF", "QLoRA"],
            seeAlso: ["/learn/self-hosting/quantization"]
        },
        
        // === R ===
        {
            term: "RAG (Retrieval-Augmented Generation)",
            definition: "A technique that enhances LLM responses by retrieving relevant information from external knowledge sources before generating an answer. Reduces hallucinations and enables access to current or private data.",
            category: "rag",
            relatedTerms: ["Vector Database", "Embedding", "Chunking", "Retrieval"],
            seeAlso: ["/learn/rag/what-is-rag"]
        },
        {
            term: "Rate Limiting",
            definition: "Restrictions on API usage, typically measured in requests per minute (RPM) or tokens per minute (TPM). Exceeding limits results in errors.",
            category: "infrastructure",
            relatedTerms: ["API Key", "Token", "Throttling"],
            seeAlso: ["/learn/llm-fundamentals/intro-to-llms/first-api-call"]
        },
        {
            term: "ReAct",
            definition: "Reasoning and Acting. A prompting framework where LLMs alternate between reasoning (thinking) and acting (using tools), enabling more complex problem-solving.",
            category: "agents",
            relatedTerms: ["Agent", "Chain-of-Thought", "Tool Use"],
            seeAlso: ["/learn/agents/react-loop"]
        },
        {
            term: "Reranking",
            definition: "A second-stage retrieval step that reorders initial search results using a more sophisticated model, improving relevance of final results.",
            category: "rag",
            relatedTerms: ["RAG", "Dense Retrieval", "Cross-Encoder"],
            seeAlso: ["/learn/rag/reranking"]
        },
        {
            term: "RLHF",
            definition: "Reinforcement Learning from Human Feedback. A training technique where models are fine-tuned using human preferences to align outputs with human values and expectations.",
            category: "training",
            relatedTerms: ["Fine-tuning", "Constitutional AI", "Alignment"],
            seeAlso: ["/learn/fine-tuning"]
        },
        
        // === S ===
        {
            term: "Self-Attention",
            definition: "A mechanism where each position in a sequence attends to all other positions, allowing the model to capture relationships regardless of distance. The core of transformer architecture.",
            category: "architecture",
            relatedTerms: ["Attention Mechanism", "Transformer", "Multi-Head Attention"],
            seeAlso: ["/learn/advanced-llm/attention-mechanisms"]
        },
        {
            term: "Semantic Search",
            definition: "Search based on meaning rather than exact keyword matching. Uses embeddings to find conceptually similar content even with different wording.",
            category: "embeddings",
            relatedTerms: ["Embedding", "Vector Database", "Cosine Similarity"],
            seeAlso: ["/learn/embeddings-vectors/semantic-search-lab"]
        },
        {
            term: "Streaming",
            definition: "Receiving LLM output token-by-token as it's generated, rather than waiting for the complete response. Improves perceived latency in user interfaces.",
            category: "basics",
            relatedTerms: ["Token", "Completion", "SSE"],
            seeAlso: ["/learn/llm-fundamentals/intro-to-llms/first-api-call"]
        },
        {
            term: "System Prompt",
            definition: "Initial instructions given to an LLM that define its role, behavior, and constraints. Persists throughout the conversation and guides all responses.",
            category: "prompting",
            relatedTerms: ["Prompt", "Prompt Engineering", "Role"],
            seeAlso: ["/learn/llm-fundamentals/prompt-engineering/prompt-fundamentals"]
        },
        
        // === T ===
        {
            term: "Temperature",
            definition: "A parameter controlling randomness in LLM outputs. Lower values (0-0.3) produce more deterministic responses; higher values (0.7-1.0) increase creativity and variation.",
            category: "parameters",
            relatedTerms: ["Top-p", "Top-k", "Sampling"],
            seeAlso: ["/learn/llm-fundamentals/parameters/temperature-top-p"]
        },
        {
            term: "Token",
            definition: "The basic unit of text processing in LLMs. Can be a word, part of a word, or punctuation. Most models use ~4 characters per token on average for English.",
            category: "tokens",
            relatedTerms: ["Tokenizer", "Context Window", "BPE"],
            seeAlso: ["/learn/llm-fundamentals/tokens/what-are-tokens"]
        },
        {
            term: "Tokenizer",
            definition: "The component that converts text into tokens for the model. Different models use different tokenizers (GPT uses tiktoken, Llama uses SentencePiece).",
            category: "tokens",
            relatedTerms: ["Token", "BPE", "SentencePiece"],
            seeAlso: ["/learn/llm-fundamentals/tokens"]
        },
        {
            term: "Tool Use",
            definition: "The ability of LLMs to call external tools, APIs, or functions to accomplish tasks beyond text generation. Enables web search, calculations, database queries, etc.",
            category: "agents",
            relatedTerms: ["Function Calling", "Agent", "MCP"],
            seeAlso: ["/learn/agents/tool-use"]
        },
        {
            term: "Top-k",
            definition: "A sampling parameter that limits token selection to the k most likely next tokens. Lower values make output more focused; higher values allow more variety.",
            category: "parameters",
            relatedTerms: ["Top-p", "Temperature", "Sampling"],
            seeAlso: ["/learn/llm-fundamentals/parameters/temperature-top-p"]
        },
        {
            term: "Top-p (Nucleus Sampling)",
            definition: "A sampling parameter that selects from the smallest set of tokens whose cumulative probability exceeds p. More dynamic than top-k as the number of considered tokens varies.",
            category: "parameters",
            relatedTerms: ["Top-k", "Temperature", "Sampling"],
            seeAlso: ["/learn/llm-fundamentals/parameters/temperature-top-p"]
        },
        {
            term: "Transformer",
            definition: "The neural network architecture behind modern LLMs, introduced in 'Attention Is All You Need' (2017). Uses self-attention mechanisms to process sequences in parallel.",
            category: "architecture",
            relatedTerms: ["Attention Mechanism", "Self-Attention", "GPT"],
            seeAlso: ["/learn/llm-fundamentals/intro-to-llms/how-they-work"]
        },
        
        // === V ===
        {
            term: "Vector Database",
            definition: "A database optimized for storing and querying high-dimensional vectors (embeddings). Examples include ChromaDB, Pinecone, Weaviate, and Milvus.",
            category: "embeddings",
            relatedTerms: ["Embedding", "RAG", "Semantic Search"],
            seeAlso: ["/learn/embeddings-vectors/vector-db-intro"]
        },
        {
            term: "vLLM",
            definition: "A high-performance inference engine for LLMs that uses PagedAttention for efficient memory management. Significantly faster than standard inference for production deployments.",
            category: "infrastructure",
            relatedTerms: ["Self-hosting", "Inference", "Ollama"],
            seeAlso: ["/learn/self-hosting/vllm"]
        },
        
        // === Z ===
        {
            term: "Zero-shot Learning",
            definition: "Using an LLM to perform a task without any examples, relying solely on instructions. The model applies its pre-trained knowledge to new tasks.",
            category: "prompting",
            relatedTerms: ["One-shot Learning", "Few-shot Learning", "In-Context Learning"],
            seeAlso: ["/learn/llm-fundamentals/prompt-engineering/zero-few-shot"]
        },
        {
            term: "Zero-shot CoT",
            definition: "Zero-shot Chain-of-Thought. Adding 'Let's think step by step' to prompts to trigger reasoning without providing examples. Simple but effective technique.",
            category: "prompting",
            relatedTerms: ["Chain-of-Thought", "Zero-shot Learning", "Prompt Engineering"],
            seeAlso: ["/learn/llm-fundamentals/prompt-engineering"]
        }
    ],
    
    categories: {
        basics: { name: "Basics", icon: "ðŸ“š", color: "blue" },
        prompting: { name: "Prompting", icon: "âœï¸", color: "green" },
        tokens: { name: "Tokens", icon: "ðŸ”¤", color: "yellow" },
        parameters: { name: "Parameters", icon: "âš™ï¸", color: "purple" },
        embeddings: { name: "Embeddings & Vectors", icon: "ðŸ”¢", color: "cyan" },
        rag: { name: "RAG", icon: "ðŸ“š", color: "orange" },
        agents: { name: "Agents", icon: "ðŸ¤–", color: "red" },
        training: { name: "Training & Fine-tuning", icon: "ðŸŽ¯", color: "pink" },
        architecture: { name: "Architecture", icon: "ðŸ—ï¸", color: "indigo" },
        models: { name: "Models", icon: "ðŸ§ ", color: "emerald" },
        infrastructure: { name: "Infrastructure", icon: "ðŸ”§", color: "slate" },
        frameworks: { name: "Frameworks", icon: "ðŸ“¦", color: "amber" },
        challenges: { name: "Challenges", icon: "âš ï¸", color: "rose" }
    }
};

class GlossarySystem {
    constructor() {
        this.terms = GlossaryData.terms;
        this.categories = GlossaryData.categories;
        this.filteredTerms = [...this.terms];
        this.activeCategory = null;
        this.searchQuery = '';
    }
    
    // Search terms
    search(query) {
        this.searchQuery = query.toLowerCase();
        this.applyFilters();
        return this.filteredTerms;
    }
    
    // Filter by category
    filterByCategory(category) {
        this.activeCategory = category;
        this.applyFilters();
        return this.filteredTerms;
    }
    
    // Apply both search and category filters
    applyFilters() {
        this.filteredTerms = this.terms.filter(term => {
            const matchesSearch = !this.searchQuery || 
                term.term.toLowerCase().includes(this.searchQuery) ||
                term.definition.toLowerCase().includes(this.searchQuery) ||
                term.relatedTerms.some(t => t.toLowerCase().includes(this.searchQuery));
            
            const matchesCategory = !this.activeCategory || term.category === this.activeCategory;
            
            return matchesSearch && matchesCategory;
        });
    }
    
    // Get term by name
    getTerm(termName) {
        return this.terms.find(t => t.term.toLowerCase() === termName.toLowerCase());
    }
    
    // Get terms grouped by first letter
    getGroupedTerms() {
        const grouped = {};
        this.filteredTerms.forEach(term => {
            const letter = term.term[0].toUpperCase();
            if (!grouped[letter]) {
                grouped[letter] = [];
            }
            grouped[letter].push(term);
        });
        
        // Sort each group alphabetically
        Object.keys(grouped).forEach(letter => {
            grouped[letter].sort((a, b) => a.term.localeCompare(b.term));
        });
        
        return grouped;
    }
    
    // Get all categories with term counts
    getCategoriesWithCounts() {
        const counts = {};
        this.terms.forEach(term => {
            counts[term.category] = (counts[term.category] || 0) + 1;
        });
        
        return Object.entries(this.categories).map(([key, cat]) => ({
            id: key,
            ...cat,
            count: counts[key] || 0
        }));
    }
    
    // Generate tooltip HTML for inline term
    getTooltipHTML(termName) {
        const term = this.getTerm(termName);
        if (!term) return null;
        
        const cat = this.categories[term.category];
        return `
            <div class="glossary-tooltip">
                <div class="font-semibold">${term.term}</div>
                <div class="text-xs text-slate-400 mb-1">${cat.icon} ${cat.name}</div>
                <div class="text-sm">${term.definition}</div>
                ${term.relatedTerms.length > 0 ? `
                    <div class="mt-2 text-xs text-slate-400">
                        Related: ${term.relatedTerms.slice(0, 3).join(', ')}
                    </div>
                ` : ''}
            </div>
        `;
    }
    
    // Get random term (for "Term of the Day" feature)
    getRandomTerm() {
        return this.terms[Math.floor(Math.random() * this.terms.length)];
    }
    
    // Get terms for a specific learning module
    getTermsForModule(modulePath) {
        return this.terms.filter(term => 
            term.seeAlso.some(link => link.includes(modulePath))
        );
    }
}

// Initialize glossary system
const glossarySystem = new GlossarySystem();

// Compute all available letters once at module load
const allAvailableLetters = (() => {
    const letters = new Set();
    GlossaryData.terms.forEach(term => {
        letters.add(term.term[0].toUpperCase());
    });
    return Array.from(letters).sort();
})();

// Alpine.js component for glossary page
document.addEventListener('alpine:init', () => {
    Alpine.data('glossaryPage', () => ({
        searchQuery: '',
        activeCategory: null,
        activeLetter: null,
        groupedTerms: {},
        categories: [],
        selectedTerm: null,
        showModal: false,
        
        init() {
            this.categories = glossarySystem.getCategoriesWithCounts();
            this.updateTerms();
        },
        
        search(query) {
            this.searchQuery = query;
            glossarySystem.search(query);
            this.updateTerms();
        },
        
        filterCategory(category) {
            this.activeCategory = this.activeCategory === category ? null : category;
            glossarySystem.filterByCategory(this.activeCategory);
            this.updateTerms();
        },
        
        filterByLetter(letter) {
            this.activeLetter = this.activeLetter === letter ? null : letter;
            this.updateTerms();
        },
        
        clearAllFilters() {
            this.searchQuery = '';
            this.activeCategory = null;
            this.activeLetter = null;
            glossarySystem.search('');
            glossarySystem.filterByCategory(null);
            this.updateTerms();
        },
        
        getAllLetters() {
            // Return the pre-computed list of all letters that have terms
            return allAvailableLetters;
        },
        
        updateTerms() {
            let grouped = glossarySystem.getGroupedTerms();
            
            // Apply letter filter if active
            if (this.activeLetter) {
                const filtered = {};
                if (grouped[this.activeLetter]) {
                    filtered[this.activeLetter] = grouped[this.activeLetter];
                }
                grouped = filtered;
            }
            
            this.groupedTerms = grouped;
        },
        
        openTerm(term) {
            this.selectedTerm = term;
            this.showModal = true;
        },
        
        closeModal() {
            this.showModal = false;
            this.selectedTerm = null;
        },
        
        getCategoryInfo(categoryId) {
            return glossarySystem.categories[categoryId] || { name: categoryId, icon: 'ðŸ“„', color: 'gray' };
        },
        
        getCategoryColor(categoryId) {
            const colors = {
                blue: 'bg-blue-500/20 text-blue-400 border-blue-500/30',
                green: 'bg-green-500/20 text-green-400 border-green-500/30',
                yellow: 'bg-yellow-500/20 text-yellow-400 border-yellow-500/30',
                purple: 'bg-purple-500/20 text-purple-400 border-purple-500/30',
                cyan: 'bg-cyan-500/20 text-cyan-400 border-cyan-500/30',
                orange: 'bg-orange-500/20 text-orange-400 border-orange-500/30',
                red: 'bg-red-500/20 text-red-400 border-red-500/30',
                pink: 'bg-pink-500/20 text-pink-400 border-pink-500/30',
                indigo: 'bg-indigo-500/20 text-indigo-400 border-indigo-500/30',
                emerald: 'bg-emerald-500/20 text-emerald-400 border-emerald-500/30',
                slate: 'bg-slate-500/20 text-slate-400 border-slate-500/30',
                amber: 'bg-amber-500/20 text-amber-400 border-amber-500/30',
                rose: 'bg-rose-500/20 text-rose-400 border-rose-500/30'
            };
            const cat = glossarySystem.categories[categoryId];
            return colors[cat?.color] || colors.slate;
        },
        
        getLetters() {
            return Object.keys(this.groupedTerms).sort();
        },
        
        scrollToLetter(letter) {
            const element = document.getElementById(`letter-${letter}`);
            if (element) {
                element.scrollIntoView({ behavior: 'smooth', block: 'start' });
            }
        }
    }));
});

// Inline tooltip initialization for lesson pages
function initInlineGlossary() {
    // Find all elements with data-glossary attribute
    document.querySelectorAll('[data-glossary]').forEach(el => {
        const termName = el.getAttribute('data-glossary');
        const term = glossarySystem.getTerm(termName);
        
        if (term) {
            el.classList.add('glossary-term-inline');
            el.style.cursor = 'help';
            el.style.borderBottom = '1px dotted currentColor';
            
            // Create tooltip on hover
            el.addEventListener('mouseenter', (e) => {
                showGlossaryTooltip(e.target, term);
            });
            
            el.addEventListener('mouseleave', () => {
                hideGlossaryTooltip();
            });
        }
    });
}

let activeTooltip = null;

function showGlossaryTooltip(element, term) {
    hideGlossaryTooltip();
    
    const cat = glossarySystem.categories[term.category];
    const tooltip = document.createElement('div');
    tooltip.className = 'glossary-tooltip-popup fixed z-50 max-w-sm bg-slate-800 border border-slate-600 rounded-lg p-4 shadow-xl';
    tooltip.innerHTML = `
        <div class="font-semibold text-white mb-1">${term.term}</div>
        <div class="text-xs text-slate-400 mb-2">${cat.icon} ${cat.name}</div>
        <div class="text-sm text-slate-300">${term.definition}</div>
        ${term.relatedTerms.length > 0 ? `
            <div class="mt-3 pt-2 border-t border-slate-700">
                <div class="text-xs text-slate-500 mb-1">Related terms:</div>
                <div class="flex flex-wrap gap-1">
                    ${term.relatedTerms.slice(0, 4).map(t => 
                        `<span class="text-xs px-2 py-0.5 bg-slate-700 rounded text-slate-300">${t}</span>`
                    ).join('')}
                </div>
            </div>
        ` : ''}
        ${term.seeAlso.length > 0 ? `
            <a href="${term.seeAlso[0]}" class="mt-2 inline-block text-xs text-primary-400 hover:text-primary-300">
                Learn more â†’
            </a>
        ` : ''}
    `;
    
    document.body.appendChild(tooltip);
    
    // Position tooltip
    const rect = element.getBoundingClientRect();
    const tooltipRect = tooltip.getBoundingClientRect();
    
    let top = rect.bottom + 8;
    let left = rect.left;
    
    // Adjust if tooltip goes off screen
    if (left + tooltipRect.width > window.innerWidth) {
        left = window.innerWidth - tooltipRect.width - 16;
    }
    if (top + tooltipRect.height > window.innerHeight) {
        top = rect.top - tooltipRect.height - 8;
    }
    
    tooltip.style.top = `${top}px`;
    tooltip.style.left = `${left}px`;
    
    activeTooltip = tooltip;
}

function hideGlossaryTooltip() {
    if (activeTooltip) {
        activeTooltip.remove();
        activeTooltip = null;
    }
}

// Initialize on page load
document.addEventListener('DOMContentLoaded', initInlineGlossary);

// Export for use in other modules
window.GlossarySystem = GlossarySystem;
window.glossarySystem = glossarySystem;
